{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE888 Assignment 2 for Demo\n",
    "\n",
    "This code is written to demonstarte a classifier using an autoencoder for ce888 assignment 2.\n",
    "\n",
    "**Author**          : Tomoko Ayakawa<br> \n",
    "**Created on**      : 29 March 2019<br> \n",
    "**Last modified on**: 20 April 2019<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import open source libraries\n",
    "import numpy as np\n",
    "import sys, keras\n",
    "\n",
    "# import original libraries\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"./sub\")\n",
    "\n",
    "from conf import myVariables as VAR\n",
    "import load_data as DATA\n",
    "import preprocess as PREP\n",
    "import autoencoder as AE\n",
    "import mlp as MLP\n",
    "import grid_search as GS\n",
    "import comparison as CMP\n",
    "\n",
    "# Automatically reload changed modules before executing each line\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "### 1-1. Select the data to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'human activity', 1: 'spam', 2: 'phishing'}0\n",
      "Number of NaN: 0\n",
      "Data shape:  (7352, 562)\n",
      "Target labels: ['1' '2' '3' '4' '5' '6']\n",
      "Class distribution: ['0.17', '0.15', '0.13', '0.17', '0.19', '0.19']\n"
     ]
    }
   ],
   "source": [
    "data_list={0: \"human activity\", 1: \"spam\", 2: \"phishing\"}\n",
    "data_id=int(input(data_list))\n",
    "\n",
    "col_names, features_df, targets_df, data_df, timestamp=DATA.load_data(data_id=data_id)\n",
    "unique_labels=DATA.verify_data(data_df, targets_df, dispaly_range=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Obtain small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of samples to use (Total: 7352): \n",
      " - Number of features: 561\n",
      " - Minimul data size : 4538\n",
      " - Specified size    : 4538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features, classes=PREP.get_small_data(features_df, targets_df)\n",
    "pic_file=timestamp+\"(%d)\" % len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Pre-process the data\n",
    "Fit the scaler and transform the data.<br>\n",
    "When the scaler ID is not specified, MinMaxScaler will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the scaler 0 (None), 1 (MinMax), 2 (Quantile), 3 (Standard): \n",
      "Scaler: MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "scl, features_nrm=PREP.pre_processing(features, display_result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the test_size (0<=test_size<1): \n",
      "Training: 3630, Test: 908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y=features_nrm, classes\n",
    "X_tr, X_te, y_tr, y_te=PREP.split_data(features_nrm, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build an Autoencoder\n",
    "### 4-1. Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define 9 parameters for the autoencoder.\n",
      "When skipped (push enter) or an invalid value is given, the default value will be used.\n",
      "[Parameter 1/9: Autoencoder type] 0:Basic or 1:Stacked (default=1): \n",
      "[Parameter 2/9: Number of neurons in each layer] Integers separated by comma (default=500,400,300,200,100,50,25,10): \n",
      "[Parameter 3/9: Activation function] 0:relu 1:sigmoid 2:tanh 3:softmax (default=relu): \n",
      "[Parameter 4/9: Optimiser] 0:adam 1:sdg (default=adam): \n",
      " - Learning rate (default=0.001000): \n",
      "[Parameter 5/9: Loss function] 0:mse 1:mean_absolute_error 2:mean_squared_logarithmic_error 3:categorical_crossentropy (default=mse): \n",
      "[Parameter 6/9: Dropout rate] 0<=rate<1 (default=0.000000): \n",
      "[Parameter 7/9: Training epochs] (default=100): \n",
      "[Parameter 8/9: Verbose] 0:False or 1:True (default=0): \n",
      "[Parameter 9/9: Summary display] 0:False or 1:True (default=0): \n",
      "\n",
      "Parameters for the autoencoder are\n",
      " 1. Mode: 1(Stacked)\n",
      " 2. Layers: [500, 400, 300, 200, 100, 50, 25, 10]\n",
      " 3. Activation function: relu\n",
      " 4. Optimiser: <keras.optimizers.Adam object at 0x137bc1978>\n",
      " 5. Loss function: mse\n",
      " 6. Dropout: 0.000000\n",
      " 7. Epochs: 100\n",
      " 8. Verbose: 0(False)\n",
      " 9. Summary_display: 0(False)\n"
     ]
    }
   ],
   "source": [
    "ae_layers, mode, act, opt, loss, dropout, \\\n",
    "    epochs, verbose, summary_display=AE.get_parameters(data_id)\n",
    "pic_file=timestamp+\"(%d_%.1f)\" % (len(features), dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1/8 ...\n"
     ]
    }
   ],
   "source": [
    "encoder, histories=AE.autoencoder(X, layers=ae_layers, mode=mode, act=act, opt=opt, \n",
    "                   loss=loss, dropout=dropout, epochs=epochs, verbose=verbose, \n",
    "                   summary_display=summary_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Display the training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE.plot_ae_loss_history(histories, mode, pic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Extract features from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_cmp=encoder.predict(X)\n",
    "X_tr_cmp=encoder.predict(X_tr)\n",
    "X_te_cmp=encoder.predict(X_te)\n",
    "\n",
    "print(\"The number of compressed features:\", len(X_all_cmp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a discriminative neural network\n",
    "\n",
    "### 5-1. Grid Search for optimal parameter\n",
    "#### (1) Define the parameters\n",
    "|parameter               |default    |description                                              |\n",
    "|------------------------|-----------|---------------------------------------------------------|\n",
    "|hidden_layer_sizes      |(100,)     |i-th element: numb of neurons in the i-th hidden layer   |\n",
    "|activation              |\"relu\"     |activation function                                      |\n",
    "|solver                  |\"adam\"     |weight optimasation                                      |\n",
    "|learning_rate_init      |0.001      |step-size in weights update (solver=’sgd’ or ‘adam’)     |\n",
    "|max_iter                |200        |max num of iterations                                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act, h_num, max_itr, lr, solver, splits=GS.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid, clf=GS.parameter_grid(activation=act, \\\n",
    "#                    hidden_layer_sizes=(h_num,), max_iter=max_itr, \\\n",
    "#                    learning_rate_init=lr, solver=solver)\n",
    "#if param_grid!=1: GS.grid_search(X_all_cmp, y, clf, param_grid, grid_splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Define the parameters for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune, h_num, h_act, out_act, opt, loss, epochs, val_rate, \\\n",
    "    verbose, summary_display=MLP.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLP.build_mlp(encoder, num_in=ae_layers[-1], num_out=len(unique_labels), \\\n",
    "            finetune=finetune, h_num=h_num, h_act=h_act, \\\n",
    "            out_act=out_act, opt=opt, loss=loss, \\\n",
    "            summary_display=summary_display)\n",
    "histories=MLP.train_mlp(X, y, model, epochs=epochs, \\\n",
    "            val_rate=val_rate, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. Display the training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.plot_mlp_loss_history(histories, pic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "### 6-1. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, fscores=MLP.cross_validation(model, X, y, unique_labels, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. Train, predict and evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=MLP.evaluation(model, X_tr, y_tr, X_te, y_te, unique_labels, pic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with other thechniques\n",
    "### 7-1. Performance comparison with DT, NB, SVM, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# original data (uncompressed)\n",
    "CMP.performance_comparison(pred, X_tr, X_te, y_tr, y_te, \\\n",
    "                           timestamp+\"(%d)\" % len(features), data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2. Plot 3 dimensions obtained by PCA and Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the compressed features have more than three dimensions, retrain the autoencoder\n",
    "if ae_layers[-1]!=3:\n",
    "    if input(\"Retrain the autoencoder? (y/n): \")==\"y\":\n",
    "        ae_layers.append(5)\n",
    "        ae_layers.append(3)\n",
    "        re_encoder, histories=AE.autoencoder(X, layers=ae_layers, mode=mode, act=act, \n",
    "                           opt=opt, loss=loss, dropout=dropout, epochs=epochs, \n",
    "                           verbose=verbose, summary_display=summary_display)\n",
    "        X_all_cmp=re_encoder.predict(X)\n",
    "\n",
    "        print(\"The number of compressed features:\", len(X_all_cmp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of unirue values in each dimension\n",
    "for x, t in zip([X, X_all_cmp], [\"PCA    \", \"Encoder\"]):\n",
    "    print(t, \":\", len(x), \n",
    "          \"\\t| axis-1:\", len(np.unique(x[:,0])), \n",
    "          \"\\t| axis-2:\", len(np.unique(x[:,1])),\n",
    "          \"\\t| axis-3:\", len(np.unique(x[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "if ae_layers[-1]==3: CMP.plot_3D(X, X_all_cmp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== End of the code ====="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
