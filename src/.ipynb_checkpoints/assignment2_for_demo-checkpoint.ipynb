{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE888 Assignment 2 for Demo\n",
    "\n",
    "This code is written to demonstarte data analysis made for ce888 assignment 2.\n",
    "\n",
    "**Author**          : Tomoko Ayakawa<br> \n",
    "**Created on**      : 29 March 2019<br> \n",
    "**Last modified on**: 10 April 2019<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open source libraries\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# import original libraries\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "#import utility as UTL\n",
    "import assignment2_main as MAIN2\n",
    "from conf import myVariables as VAR\n",
    "import load_data as DATA\n",
    "import preprocess as PREP\n",
    "\n",
    "\n",
    "\n",
    "import histogram as HST\n",
    "import correlation as CRRL\n",
    "import pca as PCA\n",
    "import classifier as CLS\n",
    "import feature_importance as IMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "Load data from **data** directory.<br>\n",
    "\n",
    "### 1-1. Select the data to load\n",
    "> 0. Human Activity\n",
    "1. Spam\n",
    "2. Phishing\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'human activity', 1: 'spam', 2: 'phishing'}0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./sub/load_data.py:52: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  data_df = data_df.convert_objects(convert_numeric=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN: 0\n",
      "Data shape:  (7352, 562)\n",
      "Target labels: ['1' '2' '3' '4' '5' '6']\n",
      "Class distribution: ['0.17', '0.15', '0.13', '0.17', '0.19', '0.19']\n"
     ]
    }
   ],
   "source": [
    "data_list={0: \"human activity\", 1: \"spam\", 2: \"phishing\"}\n",
    "data_id=int(input(data_list))\n",
    "\n",
    "col_names, features_df, targets_df, data_df, pic_file=DATA.load_data(data_id=data_id)\n",
    "unique_labels=DATA.verify_data(data_df, targets_df, dispaly_range=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Obtain the small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of samples to use (Total: 7352): 100\n",
      " - Number of features: 561\n",
      " - Minimul data size : 4538\n",
      " - Specified size    : 100\n"
     ]
    }
   ],
   "source": [
    "features, classes=PREP.get_small_data(features_df, targets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Pre-process the data\n",
    "Fit the scaler and transform the data.<br>\n",
    "When the scaler ID is not specified, MinMaxScaler will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the scaler 0 (None), 1 (MinMax), 2 (Quantile), 3 (Standard): 0\n",
      "Scaler: None\n"
     ]
    }
   ],
   "source": [
    "scl, features_nrm=PREP.pre_processing(features, display_result=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the test_size (0<=test_size<1): 0.2\n",
      "Training: 80, Test: 20\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_te, y_tr, y_te=split_data(features_nrm, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build an Autoencoder\n",
    "### 4-1. Define the parameters\n",
    "- `num_outs`: number of outputs\n",
    "- `act_funcs`:activation functions\n",
    "- `opt`:optimiser\n",
    "- `loss`: loss function\n",
    "- `tr_epoch`: training epoch\n",
    "- `ver`: verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Build an autoencoder\n",
    "#### (1) Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(X, epochs=20, dropout=20, verbose=1, summary_display=False): \n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Input, Dropout\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.advanced_activations import PReLU\n",
    "    import warnings\n",
    "    \n",
    "    #ignore warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    #create an input holder\n",
    "    num_features=len(X[0])\n",
    "    input_holder=Input(shape=(num_features,))\n",
    "        \n",
    "    #initialise the encoder variables\n",
    "    num_outs.insert (0, num_features)\n",
    "    act_funcs.insert (0, \"Inputs\")\n",
    "    tmp_holder = input_holder\n",
    "\n",
    "    #initialise the decoder variables\n",
    "    n=len(num_outs)\n",
    "    num_out_dec=[]\n",
    "    for i in range(n-1, -1, -1): num_out_dec.append(num_outs[i])     \n",
    "    num_out_dec.append(num_features)\n",
    "    act_funcs_dec=[act_funcs[-1]]*len(num_out_dec)\n",
    "    \n",
    "    print(\"num_outs:\", num_outs, num_out_dec)\n",
    "    print(\"act:\", act_funcs, act_funcs_dec)\n",
    "    #build an encoder\n",
    "    encode = input_holder\n",
    "    for i in range (0, n-1):\n",
    "        print(\"i=\",i,\"| output=\", num_outs[i+1], \", input=\",num_outs[i],\", act=\",act_funcs[i+1])\n",
    "        \n",
    "        encode=Dense(num_outs[i+1], input_dim=num_outs[i], \\\n",
    "                    activation = act_funcs[i+1])(encode)\n",
    "        if dropout!=0: \n",
    "            print(\"Add droppout\")\n",
    "            encode=Dropout(dropout)(encode)\n",
    "            \n",
    "    decode = encode\n",
    "    for i in range (0, len(act_funcs)-1):\n",
    "        print(\"i=\",i,\"| output=\", num_out_dec[i+1], \", input=\",num_out_dec[i],\", act=\",act_funcs_dec[i+1])\n",
    "        decode = Dense ( num_out_dec[i+1], input_dim=num_out_dec[i], \\\n",
    "                        activation=act_funcs_dec[i+1])(decode)\n",
    "        if dropout!=0:\n",
    "            if i<len(act_funcs)-2:\n",
    "                print(\"Add droppout\")\n",
    "                decode=Dropout(dropout)(decode)\n",
    "                \n",
    "    autoencoder=Model(input=input_holder, output=decode)\n",
    "    autoencoder.compile (optimizer=opt, loss=loss)\n",
    "    \n",
    "    if summary_display:\n",
    "        print(autoencoder.summary())\n",
    "        #from keras.utils import plot_model\n",
    "        #plot_model(autoencoder, show_shapes=True, to_file='%sautoencoder.png' % VAR.out_path)\n",
    "    \n",
    "    #train the autoencoder\n",
    "    loss_hiss=autoencoder.fit (X, X, epochs=epochs, verbose=verbose)\n",
    "    \n",
    "    return encode, [loss_hiss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot==1.2.3\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_autoencoder(X, epochs=20, dropout=0, verbose=1, summary_display=False): \n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Dense, Input, Dropout\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.advanced_activations import PReLU\n",
    "    import warnings\n",
    "    \n",
    "    # ignore warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    #create an input holder\n",
    "    num_features=len(X[0])\n",
    "    #input_holder=Input(shape=(num_features,))\n",
    "        \n",
    "    #initialise the variables\n",
    "    num_outs.insert(0, num_features)\n",
    "    tmp_holder=Input(shape=(num_features,))\n",
    "    tr_data=X\n",
    "    encoders = []\n",
    "    loss_hiss = []\n",
    "\n",
    "    #train encoder layers\n",
    "    for i in range(len(act_funcs)):\n",
    "        print(\"Training Layer %d/%d ...\" % (i+1, len(act_funcs)))\n",
    "        print(\"i=\",i,\"| output=\", num_outs[i+1], \", input=\",num_outs[i],\", act=\",act_funcs [i])\n",
    "        \n",
    "        encode=Dense(num_outs[i+1], input_dim=num_outs[i], \\\n",
    "                       activation=act_funcs [i])(tmp_holder)\n",
    "        if dropout!=0: \n",
    "            print(\"Add droppout\")\n",
    "            encode=Dropout(dropout)(encode)\n",
    "            \n",
    "        decode=Dense(num_outs[i], input_dim=num_outs[i+1], \\\n",
    "                       activation=act_funcs[i])(encode)\n",
    "        encoder=Model(input=tmp_holder, output=decode)\n",
    "        encoder.compile(optimizer=opt, loss=loss)\n",
    "    \n",
    "        # train a layer\n",
    "        loss_his=encoder.fit(tr_data, tr_data, epochs=epochs, verbose=verbose)\n",
    "        loss_hiss.append(loss_his)\n",
    "        \n",
    "        # use output of the layer as next training input\n",
    "        encoder=Model(input=tmp_holder, output=encode)\n",
    "        tr_data=encoder.predict(tr_data)\n",
    "        \n",
    "        # update the input_holder\n",
    "        tmp_holder=Input(shape=(num_outs[i+1],))\n",
    "    \n",
    "        # store the trainined layer in a list\n",
    "        encoders.append(encoder)\n",
    "        \n",
    "    # connect traind encoder layers as 'encode'\n",
    "    model=Sequential()\n",
    "    for e in encoders:\n",
    "        #print(e.summary())\n",
    "        model.add(e)\n",
    "        print(model.summary())\n",
    "    \n",
    "    if summary_display:\n",
    "        #print(model.summary())\n",
    "        from keras.utils import plot_model\n",
    "        try:\n",
    "            plot_model(model, show_shapes=True, to_file='%sstackedae.png' % VAR.out_path)\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "    return model, loss_hiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "num_outs=[55, 30, 10]\n",
    "act_funcs=[\"relu\"] * len(num_outs) \n",
    "opt=Adam(lr=0.001)\n",
    "loss=\"mse\"\n",
    "tr_epoch=10\n",
    "ver=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ae_epoch=20     # training epoch for autoencoders\n",
    "    ae_dropout=0    # dropout rate for autoencoders\n",
    "    ae_lr={0: 0.001, 1: 0.001, 2: 0.001}  # learning rate\n",
    "    ae_layers={0: [55, 30, 10],  # number of neurons of each layer\n",
    "               1: [55, 30, 10], \n",
    "               2: [55, 30, 10]} \n",
    "    ae_act={0: \"relu\", 1: \"relu\", 2: \"relu\"} # activation function\n",
    "    ae_loss=\"mse\" # loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_outs: [561, 55, 30, 10] [10, 30, 55, 561, 561]\n",
      "act: ['Inputs', 'relu', 'relu', 'relu'] ['relu', 'relu', 'relu', 'relu', 'relu']\n",
      "i= 0 | output= 55 , input= 561 , act= relu\n",
      "Add droppout\n",
      "i= 1 | output= 30 , input= 55 , act= relu\n",
      "Add droppout\n",
      "i= 2 | output= 10 , input= 30 , act= relu\n",
      "Add droppout\n",
      "i= 0 | output= 30 , input= 10 , act= relu\n",
      "Add droppout\n",
      "i= 1 | output= 55 , input= 30 , act= relu\n",
      "Add droppout\n",
      "i= 2 | output= 561 , input= 55 , act= relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 561)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                30910     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                1680      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 55)                1705      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 561)               31416     \n",
      "=================================================================\n",
      "Total params: 66,351\n",
      "Trainable params: 66,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "encoder, loss_hiss=autoencoder(X_tr, epochs=tr_epoch, dropout=0.2, verbose=ver, summary_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e3636d362505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_outs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcudnn_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudnnGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudnnLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudnnRNNRelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_cudnn_rnn_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/rnn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatic_bidirectional_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mop_def_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontrib_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbucketization_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_feature_cross_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1983\u001b[0m                      \u001b[0moff_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m                      \u001b[0moutputs_collections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m                      scope=None):\n\u001b[0m\u001b[1;32m   1986\u001b[0m   \"\"\"Transform numeric labels into onehot_labels using `tf.one_hot`.\n\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36madd_arg_scope\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_with_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \"\"\"\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Caller's name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[1;32m     88\u001b[0m                           decorator_argspec)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# stacked autoencoder\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "num_outs=[55, 30, 10]\n",
    "act_funcs=[\"relu\"] * len(num_outs) \n",
    "opt=SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "opt=\"adam\"\n",
    "loss=\"mse\"\n",
    "tr_epoch=10\n",
    "ver=0\n",
    "\n",
    "\n",
    "\n",
    "encoder, loss_hiss=stacked_autoencoder(X_tr, epochs=tr_epoch, dropout=0.2, verbose=ver, summary_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Display the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_loss_history(histories, pic_file):\n",
    "    # define grid\n",
    "    n=len(num_outs)\n",
    "    if n>3:\n",
    "        col=3\n",
    "        row=n//col+1\n",
    "    else:\n",
    "        row, col=1, n\n",
    "    \n",
    "    fig=plt.figure(figsize=(col*6, row*4))\n",
    "    \n",
    "    for i in range(len(histories)):\n",
    "        plt.subplot(row, col, i+1)\n",
    "        plt.plot(histories[i].history[\"loss\"])\n",
    "        plt.title(\"Layer %d Training Loss\" % (i+1))\n",
    "    \n",
    "    \n",
    "    plt.show ()\n",
    "    \n",
    "    fig.savefig(\"%s%s_ae_loss.png\" % (VAR.out_path, pic_file), \\\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ae_loss_history(loss_hiss, pic_file=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Extract features from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_compressed=encoder.predict(features_nrm)\n",
    "X_tr_compressed=encoder.predict(X_tr)\n",
    "X_tr_compressed=encoder.predict(X_te)\n",
    "\n",
    "print(\"The number of compressed features:\", len(feat_compressed[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a discriminative neural network\n",
    "### 5-1. Keras Multiclass classifier\n",
    "This model is developed in reference to [Multi-Class Classification Tutorial with the Keras Deep Learning Library ](https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_act='relu'\n",
    "out_act='softmax'\n",
    "cls_optimizer=\"adam\"\n",
    "cls_loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(num_out=1, finetune=False):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Input\n",
    "\n",
    "    model=Sequential()\n",
    "    for e in encoder.layers:\n",
    "        e.trainable=finetune\n",
    "        model.add(e)\n",
    "    model.add(Dense(30, input_dim=num_outs[-1], activation=hidden_act))\n",
    "    model.add(Dense(num_out, activation=out_act))\n",
    "\n",
    "    model.compile(loss=cls_loss, optimizer=cls_optimizer, metrics=['accuracy'])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=classifier(num_out=len(unique_labels), finetune=True)\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class(Y):\n",
    "    # For multi class task, replace the target with dummy values\n",
    "    from keras.utils import np_utils\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # encode class values as integers\n",
    "    label_encoder=LabelEncoder()\n",
    "    encoded_Y=label_encoder.fit_transform(Y)\n",
    "\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    encoded_Y=np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "    return encoded_Y\n",
    "\n",
    "Y_tr=multi_class(Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "loss_hiss=mlp.fit(X_tr, Y_tr, epochs=20, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mlp_loss_history(histories, pic_file):\n",
    "    import matplotlib.ticker as ticker\n",
    "    \n",
    "    fig=plt.figure(figsize=(15,5))\n",
    "\n",
    "    metrics=[[\"loss\", \"val_loss\"], [\"acc\", \"val_acc\"]]\n",
    "    labels=[\"Loss\", \"Accuracy\"]\n",
    "    for i, m , l in zip(range(1,3), metrics, labels):\n",
    "        plt.subplot(1, 2, i)\n",
    "        plt.plot(histories.history[m[0]], label=\"training\")\n",
    "        plt.plot(histories.history[m[1]], label=\"validation\")\n",
    "        plt.title(\"MLP Training and Validation %s\" % l)\n",
    "        plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show ()\n",
    "    \n",
    "    fig.savefig(\"%s%s_ae_loss.png\" % (VAR.out_path, pic_file), \\\n",
    "                bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_file=\"test_mlp\"\n",
    "plot_mlp_loss_history(loss_hiss, pic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Sklearn Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier=MLPClassifier(activation = \"tanh\", alpha = 1, hidden_layer_sizes = (14,), \n",
    "                   max_iter = 8000, learning_rate_init = 0.00001, \n",
    "                    solver = \"lbfgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Grid Search\n",
    "The codes and markdown below are originally written by the author for the assignment for CE802 which was submitted to the CSEE Department on 16 January 2019.<br><br>\n",
    "Major parameters to be tuned are:\n",
    "\n",
    "\n",
    "|parameter               |default    |description                                              |\n",
    "|------------------------|-----------|---------------------------------------------------------|\n",
    "|hidden_layer_sizes      |(100,)     |i-th element: numb of neurons in the i-th hidden layer   |\n",
    "|activation              |\"relu\"     |activation function                                      |\n",
    "|solver                  |\"adam\"     |weight optimasation                                      |\n",
    "|alpha                   |0.0001     |regularization parameter                                 |\n",
    "|learning_rate_init      |0.001      |step-size in weights update (solver=sgd or adam)     |\n",
    "|max_iter                |200        |max num of iterations                                    |\n",
    "|momentum                |0.9        |momentum for gradient descent update (range (0, 1), solver=sgd|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import \\\n",
    "    GridSearchCV, cross_val_score, KFold, ShuffleSplit\n",
    "from statistics import mode\n",
    "\n",
    "Hs=[(100,), (14, ), (28, ), (42, ), (56, )]\n",
    "ACTs=[\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "SLOs=[\"lbfgs\", \"sgd\", \"adam\"]\n",
    "ACTs=[\"identity\"]\n",
    "SLOs=[\"lbfgs\"]\n",
    "ALPHAs=np.logspace(-5, 1, 7)\n",
    "ITRs=np.linspace(6000, 10000, 5, dtype = int)\n",
    "ETAs=np.logspace(-5, -3, 3)\n",
    "\n",
    "grid_list=[]\n",
    "grid_list.append(dict(activation=ACTs, solver=SLOs))\n",
    "grid_list.append(dict(learning_rate_init=ETAs, alpha=ALPHAs))\n",
    "grid_list.append(dict(max_iter=ITRs, hidden_layer_sizes=Hs))\n",
    "\n",
    "grid={0:\"Activation function & Solver\\n\", 1:\"Learning rate & Momentam\\n\", 2:\"Max iteration & Hidden layer\\n\"}\n",
    "ans=int(input(grid))\n",
    "\n",
    "param_grid=grid_list[ans]\n",
    "\n",
    "print (param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1,\n",
    "                    cv=ShuffleSplit())\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X, y):\n",
    "    score, bests, lbs, means, modes=[], [], [], [], []\n",
    "    n_splits=10\n",
    "    k_fold=KFold(n_splits=n_splits)\n",
    "\n",
    "    grd = 1\n",
    "    for tr, te in k_fold.split(X):\n",
    "        print (\"\\rProgress [%s%s]\" % \n",
    "               (grd * \"# \", (n_splits-grd) * \". \"), end = \"\")\n",
    "        grd += 1\n",
    "        best = []\n",
    "\n",
    "        clf.fit (X[tr], y[tr])\n",
    "        score.append (clf.score(X[te], y[te]))\n",
    "\n",
    "        for key in param_grid.keys():\n",
    "            method = getattr(clf.best_estimator_, key)\n",
    "            best.append (method)\n",
    "            if len (lbs) != len(param_grid): lbs.append(\"Best %s\" % key)\n",
    "        best.append (score[-1])\n",
    "        bests.append (best)\n",
    "\n",
    "    lbs.append (\"Fold Accuracy\")\n",
    "\n",
    "    # append mean and mode\n",
    "    for k in range (len (bests[0])):\n",
    "        avgs = (np.array (bests)[:, k]).tolist()\n",
    "        try:    modes.append (mode (avgs))\n",
    "        except: modes.append (\"N/A\")\n",
    "\n",
    "        try:    means.append (np.mean ([float (avg) for avg in avgs]))\n",
    "        except: means.append (\"N/A\")\n",
    "\n",
    "    index=[\"Fold-%2d\" % fold for fold in(range(1, grd))]\n",
    "    for add1, add2 in zip ([[\"---\"] * (len (param_grid) + 1), means, modes], \n",
    "                           [\"---\", \"mean\", \"modes\"]):\n",
    "        bests.append (add1)\n",
    "        index.append (add2)\n",
    "    \n",
    "    return bests, index, lbs\n",
    "        \n",
    "bests, index, lbs=grid_search(X=feat_compressed, y=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best scores\n",
    "print (\"\\n\")\n",
    "d = pd.DataFrame(bests, index=index, columns=lbs)\n",
    "print (d)\n",
    "\n",
    "# export the results to a file\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "mystr = input (\">>> Enter the type of predictor (for file name): \")\n",
    "d.to_csv(\"GridSearch_%s_%s.csv\" % (mystr, now), header = lbs, \n",
    "          index = index, sep = \",\", mode = \"a\")\n",
    "print (\"[  O K  ] The result of Grid Search is exported to \"\n",
    "       \"<GridSearch_%s_%s.csv>.\" % (mystr, now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_tr, Y_tr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=classifier.predict(X_te)\n",
    "\n",
    "# for multi class tasks, decode the class attribute\n",
    "pred_df=pd.DataFrame(pred, columns=unique_labels)\n",
    "pred=pred_df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "### 6-1. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "estimator=KerasClassifier(build_fn=classifier, num_out=num_out, finetune=False, epochs=100, verbose=0)\n",
    "scores=cross_val_score(estimator, X, y, cv=5)#, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (95%% confidence interval): %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. Various evaluation metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "print (\"\\n< Classification Report >\")\n",
    "print (metrics.classification_report (cls_te, pred, digits = 5))\n",
    "\n",
    "print (\"\\n< Confusion Matrix >\")\n",
    "print (pd.DataFrame (metrics.confusion_matrix(cls_te, pred)))\n",
    "print (\"Accuracy: %f\" % metrics.accuracy_score (cls_te, pred))\n",
    "\n",
    "print (\"\\n< Kappa Statistic >\")\n",
    "print (metrics.cohen_kappa_score (cls_te, pred))\n",
    "\n",
    "CLS.plot_confusion_matrix(cls_te, pred, unique_labels, \"test\", \"NN\", \"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Classification with other procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"%s_Decision_Tree (%d)\" % (pic_file, len(X))\n",
    "true_label, pred_label, clf = CLS.train (feat_te_compressed, cls_te, 0, 0)\n",
    "#CLS.plot_confusion_matrix(true_label, pred_label, unique_labels, pic_file, title, \"Decision_Tree\")\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"%s_Naive Bayes (%d)\" % (pic_file, len(X))\n",
    "true_label, pred_label, clf = CLS.train (feat_te_compressed, cls_te, 1, 0)\n",
    "#CLS.plot_confusion_matrix(true_label, pred_label, unique_labels, pic_file, title, \"Naive Bayes\")\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = \"%s_SVM (%d)\" % (pic_file, len(X))\n",
    "true_label, pred_label, clf = CLS.train (feat_te_compressed, cls_te, 2, data_id)\n",
    "#CLS.plot_confusion_matrix(true_label, pred_label, unique_labels, pic_file, title, \"SVM\")\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
